{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\leand\\miniconda3\\envs\\datascience\\lib\\site-packages\\requests\\__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "import shutil\n",
    "import mysql.connector as msql\n",
    "from mysql.connector import Error\n",
    "from geopy.geocoders import Nominatim \n",
    "geolocator = Nominatim(user_agent=\"MakroAnalyse\")\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import psycopg2.extras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introduzca la dirección donde se encuentren los conjuntos de datos: \n"
     ]
    }
   ],
   "source": [
    "print('Introduzca la dirección donde se encuentren los conjuntos de datos: ')\n",
    "\n",
    "direccion = input()\n",
    "carpeta_datos = direccion + '\\\\'\n",
    "lista_datos = []\n",
    "for file in os.listdir(carpeta_datos):\n",
    "    if file.endswith('.csv'):\n",
    "        lista_datos.append(file)\n",
    "    if file.endswith('.json'):\n",
    "        lista_datos.append(file)\n",
    "lista_datos = sorted(lista_datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta1 = (carpeta_datos + 'product_category_name_translation.csv')\n",
    "ruta2 = (carpeta_datos + 'olist_closed_deals_dataset.csv')\n",
    "ruta3 = (carpeta_datos + 'geolocation_dataset_brazil.csv')\n",
    "ruta4 = (carpeta_datos + 'olist_products_dataset.csv')\n",
    "ruta5 = (carpeta_datos + 'olist_order_payments_dataset.csv')\n",
    "ruta6 = (carpeta_datos +  'olist_orders_dataset.csv')\n",
    "ruta7 = (carpeta_datos + 'olist_order_reviews_dataset.csv')\n",
    "ruta8 = (carpeta_datos + 'olist_order_items_dataset.csv')\n",
    "ruta9 = (carpeta_datos + 'olist_sellers_dataset.csv')\n",
    "ruta10 = (carpeta_datos + 'olist_customers_dataset.csv')\n",
    "rutaestados = (carpeta_datos + 'estados.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_categoryproduct(ruta1):\n",
    "\n",
    "    data_category_product = pd.read_csv(ruta1)\n",
    "    data_category_product.loc[71] = ['sin_categoria','NoCategory']\n",
    "    data_category_product.reset_index(inplace=True)\n",
    "    data_category_product.rename(columns={'index':'product_name_id'}, inplace=True)\n",
    "\n",
    "    data_category_product.to_csv('data_normalizada\\category_product.csv', index=False)\n",
    "\n",
    "    return data_category_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_closed(ruta2):\n",
    "    \n",
    "    data_closed = pd.read_csv(ruta2)\n",
    "    data_closed['won_date'] = pd.to_datetime(data_closed['won_date'])\n",
    "    data_closed.drop(columns=['declared_monthly_revenue', 'sr_id', 'sdr_id', 'lead_behaviour_profile', \\\n",
    "        'has_company','lead_type','has_gtin','business_type' ,'average_stock', 'declared_product_catalog_size'], inplace=True)\n",
    "    data_closed.reset_index(inplace=True)\n",
    "    data_closed.rename(columns={'index':'closed_id'}, inplace=True)\n",
    "    data_closed.dropna(inplace=True)\n",
    "\n",
    "    data_closed.to_csv('data_normalizada\\closed_deal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_geolocalizacion(ruta3, rutaestados):\n",
    "    geo = pd.read_csv(ruta3)\n",
    "    geo.drop_duplicates(inplace=True)\n",
    "    geo = geo.rename(columns={'geolocation_zip_code_prefix': 'zip_code', 'geolocation_lat':'geolatitud', \\\n",
    "        'geolocation_lng':'geolongitud', 'geolocation_city':'city', 'geolocation_state' : 'state'})\n",
    "    dfj = pd.read_json(rutaestados)\n",
    "    \n",
    "    geo = geo.merge(dfj[['nome', 'sigla']], right_on='sigla', left_on='state')\n",
    "\n",
    "    geo.rename(columns={'nome':'state_name'}, inplace=True)\n",
    "\n",
    "    geo['city'] = geo['city'].str.title()\n",
    "\n",
    "    geo = geo.drop_duplicates('zip_code')\n",
    "\n",
    "    geo = geo.reset_index(drop=True)\n",
    "\n",
    "    geo = geo[['zip_code', 'geolatitud', 'geolongitud', 'city', 'state_name']]\n",
    "\n",
    "\n",
    "    geo.loc[geo.geolongitud > 0, 'geolongitud'] = geo.geolongitud * (-1)\n",
    "    geo.loc[geo.geolatitud > 5, 'geolatitud'] = geo.geolatitud * (-1)\n",
    "\n",
    "\n",
    "    a = (geo.geolongitud < -73) | (geo.geolongitud > -34) | (geo.geolatitud < -33)\n",
    "\n",
    "    geo['full_data'] = geo.state_name + ',' + geo.city \n",
    "\n",
    "    geo.loc[a, 'gcode'] = geo[a]['full_data'].apply(geolocator.geocode)\n",
    "    geo.loc[a,'lat'] = [g.latitude for g in geo[a]['gcode']]\n",
    "    geo.loc[a,'long'] = [g.longitude for g in geo[a]['gcode']]\n",
    "    \n",
    "    geo.loc[a, 'geolatitud'] = geo[a]['lat']\n",
    "    geo.loc[a, 'geolongitud'] = geo[a]['long']\n",
    "    \n",
    "    geo.drop(columns= ['gcode', 'lat', 'long', 'full_data'], inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "    geo['zip_code'] = geo['zip_code'].astype(str) \n",
    "\n",
    "    geo.to_csv('data_normalizada\\geolocation.csv', index=False)\n",
    "\n",
    "    return geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_product(ruta4):\n",
    "    data_products = pd.read_csv(ruta4)\n",
    "    data_products.drop(columns=['product_name_lenght', 'product_description_lenght'], axis=1, inplace=True)\n",
    "    ## UTILIZAR FUNCION DE ETL_CATEGORYPRODUCT\n",
    "    datamerge = merge1\n",
    "    data_products.drop(columns=['product_photos_qty'], inplace=True)\n",
    "    data_products['product_category_name'] = data_products['product_category_name'].fillna('sin_categoria')\n",
    "    data_products = pd.merge(datamerge, data_products)\n",
    "    data_products.drop(columns='product_category_name', inplace=True)\n",
    "    data_products.drop(columns='product_name_id', inplace= True)\n",
    "\n",
    "    #Utilizamos la media para rellenar espacios nulos\n",
    "    product_weight_median = data_products['product_weight_g'].median()\n",
    "    product_length_median = data_products['product_length_cm'].median()\n",
    "    product_height_median = data_products['product_height_cm'].median()\n",
    "    product_width_median = data_products['product_width_cm'].median()\n",
    "    data_products['product_weight_g'].fillna(product_weight_median, inplace=True)\n",
    "    data_products['product_length_cm'].fillna(product_length_median, inplace=True)\n",
    "    data_products['product_height_cm'].fillna(product_height_median, inplace=True)\n",
    "    data_products['product_width_cm'].fillna(product_width_median, inplace=True)   \n",
    "    data_products = data_products.reindex(columns = ['product_id', 'product_category_name_english', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm'])\n",
    "\n",
    "    data_products.to_csv('data_normalizada\\product.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_payment(ruta5):\n",
    "    data_payment = pd.read_csv(ruta5)\n",
    "    data_payment['payment_type'] = data_payment['payment_type'].replace('boleto', 'voucher')\n",
    "\n",
    "    data_payment.reset_index(inplace=True)\n",
    "    data_payment.rename(columns={'index':'payment_id'}, inplace=True)\n",
    "    data_payment['payment_id'] = data_payment['payment_id'].astype(str)\n",
    "\n",
    "    data_payment.to_csv('data_normalizada\\payment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_order(ruta6):\n",
    "    data_order = pd.read_csv(ruta6)\n",
    "    date_cols = ['order_purchase_timestamp','order_approved_at','order_delivered_carrier_date','order_delivered_customer_date','order_estimated_delivery_date']\n",
    "\n",
    "    for j in date_cols:\n",
    "        data_order[j] = pd.to_datetime(data_order[j])\n",
    "    data_order.drop([47552], inplace= True)\n",
    "    diferencia_compra_aprobacion = data_order['order_approved_at'] - data_order['order_purchase_timestamp']\n",
    "    diferencia_aprobacion_envio = data_order['order_delivered_carrier_date'] - data_order['order_approved_at']\n",
    "\n",
    "    Q1 = diferencia_compra_aprobacion.dt.days.quantile(0.25) \n",
    "    Q3 = diferencia_compra_aprobacion.dt.days.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    BI = Q1 - 1.5*IQR\n",
    "    BS = Q3 + 1.5*IQR\n",
    "    out = (diferencia_compra_aprobacion.dt.days<BI) | (diferencia_compra_aprobacion.dt.days>BS)\n",
    "\n",
    "    promedio_demora_aprobacion = diferencia_compra_aprobacion[~out].mean()\n",
    "\n",
    "    data_order.loc[diferencia_aprobacion_envio.dt.days < 0, 'order_approved_at'] = data_order.order_purchase_timestamp + pd.Timedelta(promedio_demora_aprobacion) \n",
    "\n",
    "    diferencia_aprobacion_envio_2 = data_order['order_delivered_carrier_date'] - data_order['order_approved_at']\n",
    "\n",
    "    Q1 = diferencia_aprobacion_envio_2.dt.days.quantile(0.25) \n",
    "    Q3 = diferencia_aprobacion_envio_2.dt.days.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    BI = Q1 - 1.5*IQR\n",
    "    BS = Q3 + 1.5*IQR\n",
    "    out = (diferencia_aprobacion_envio_2.dt.days<BI) | (diferencia_aprobacion_envio_2.dt.days>BS)\n",
    "\n",
    "    promedio_demora_salida = diferencia_aprobacion_envio_2[~out].mean()\n",
    "\n",
    "    data_order.loc[diferencia_aprobacion_envio_2.dt.days < 0, 'order_delivered_carrier_date'] = data_order.order_approved_at + pd.Timedelta(promedio_demora_salida) \n",
    "\n",
    "    diferencia_salida_llegada = data_order['order_delivered_customer_date'] - data_order['order_delivered_carrier_date']\n",
    "\n",
    "    Q1 = diferencia_salida_llegada.dt.days.quantile(0.25) \n",
    "    Q3 = diferencia_salida_llegada.dt.days.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    BI = Q1 - 1.5*IQR\n",
    "    BS = Q3 + 1.5*IQR\n",
    "    out = (diferencia_salida_llegada.dt.days<BI) | (diferencia_salida_llegada.dt.days>BS)\n",
    "\n",
    "    promedio_demora_llegada = diferencia_salida_llegada[~out].mean()\n",
    "\n",
    "    data_order.loc[diferencia_salida_llegada.dt.days < 0, 'order_delivered_customer_date'] = data_order.order_delivered_carrier_date + pd.Timedelta(promedio_demora_llegada) \n",
    "    \n",
    "    data_order.loc[(data_order.order_status == 'delivered') & (data_order.order_approved_at.isnull()), 'order_approved_at'] = data_order.order_purchase_timestamp + pd.Timedelta(promedio_demora_aprobacion) \n",
    "\n",
    "    data_order.loc[(data_order.order_status == 'delivered') & (data_order.order_delivered_carrier_date.isnull()), 'order_delivered_carrier_date'] = data_order.order_approved_at + pd.Timedelta(promedio_demora_salida) \n",
    "\n",
    "    data_order.loc[(data_order.order_status == 'delivered') & (data_order.order_delivered_customer_date.isnull()), 'order_delivered_customer_date'] = data_order.order_delivered_carrier_date + pd.Timedelta(promedio_demora_llegada) \n",
    "\n",
    "    lista = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
    "\n",
    "    for i in lista:\n",
    "        data_order[i] = pd.to_datetime(data_order[i]).round('s')\n",
    "\n",
    "    data_order.replace({np.nan: None}, inplace= True)\n",
    " \n",
    "    data_order.to_csv('data_normalizada\\data_order.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_review(ruta7):\n",
    "    data_order_review = pd.read_csv(ruta7)\n",
    "    data_order_review['review_creation_date'] = pd.to_datetime(data_order_review['review_creation_date'])\n",
    "    data_order_review['review_id'] = data_order_review.index\n",
    "    data_order_review['review_id'] = data_order_review['review_id'].astype(str)\n",
    "    data_order_review.drop(columns=['review_comment_title', 'review_comment_message', 'review_answer_timestamp'], inplace=True)\n",
    "\n",
    "    data_order_review.to_csv('data_normalizada\\data_review.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_items(ruta8):\n",
    "    data_order= pd.read_csv(ruta8)\n",
    "    data_order['shipping_limit_date'] = pd.to_datetime(data_order['shipping_limit_date'])\n",
    "    data_order['percentagePF'] = ( data_order['freight_value'] * 100 ) / data_order['price']\n",
    "    data_order.rename(columns= {'order_item_id' : 'quantity'}, inplace= True)\n",
    "    data_order = data_order.reindex(columns = ['order_id', 'product_id', 'seller_id', 'shipping_limit_date', 'quantity', 'price', 'freight_value', 'percentagePF'])\n",
    "    data_order.reset_index(inplace=True)\n",
    "    data_order.rename(columns={'index':'orderitem_id'}, inplace=True)\n",
    "\n",
    "    data_order.to_csv('data_normalizada\\item.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_seller(ruta9):\n",
    "    \n",
    "    df = pd.read_csv(ruta9)\n",
    "    df.rename(columns = {'seller_zip_code_prefix': 'zip_code', 'seller_city': 'city', 'seller_state': 'state'}, inplace= True)\n",
    "    df['city'] = df['city'].str.title()\n",
    "    df['zip_code'] = df['zip_code'].astype(str)\n",
    "    dfgeo = merge_geo\n",
    "    df = df[['seller_id', 'zip_code']].merge(dfgeo[['zip_code', 'city', 'state_name']], on='zip_code')\n",
    "\n",
    "    df.to_csv('data_normalizada\\seller.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etl_customer(ruta10):\n",
    "    \n",
    "    df = pd.read_csv(ruta10)\n",
    "    dfgeo = merge_geo\n",
    "    df.rename(columns = {'customer_zip_code_prefix': 'zip_code', 'customer_city': 'city', 'customer_state': 'state'}, inplace= True)\n",
    "    df['city'] = df['city'].str.title()\n",
    "    df.rename(columns={'customer_zip_code_prefix': 'zip_code'}, inplace=True)\n",
    "    df['zip_code'] = df['zip_code'].astype(str)\n",
    "    df = df[['customer_id', 'customer_unique_id', 'zip_code']].merge(dfgeo[['zip_code', 'city', 'state_name']], on='zip_code')\n",
    "    \n",
    "    df.to_csv('data_normalizada\\customer.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizando el archivo product_category_name_translation.csv\n",
      "Normalizando el archivo olist_closed_deals_dataset.csv\n",
      "Normalizando el archivo geolocation_dataset_brazil.csv\n",
      "Normalizando el archivo olist_products_dataset.csv\n",
      "Normalizando el archivo olist_order_payments_dataset.csv\n",
      "Normalizando el archivo olist_orders_dataset.csv\n",
      "Normalizando el archivo olist_order_reviews_dataset.csv\n",
      "Normalizando el archivo olist_order_items_dataset.csv\n",
      "Normalizando el archivo olist_sellers_dataset.csv\n",
      "Normalizando el archivo olist_customers_dataset.csv\n",
      "El proceso de normalización ha finalizado exitosamente\n"
     ]
    }
   ],
   "source": [
    "funciones = [etl_categoryproduct, etl_closed, etl_geolocalizacion, etl_product, etl_payment, etl_order, etl_review, etl_items, etl_seller, etl_customer]\n",
    "rutas = [ruta1, ruta2, ruta3, ruta4, ruta5, ruta6, ruta7, ruta8, ruta9, ruta10]\n",
    "j = 0\n",
    "p = 0\n",
    "\n",
    "for i in funciones:\n",
    "   print('Normalizando el archivo', rutas[j].split(carpeta_datos)[1])\n",
    "   \n",
    "   if i == etl_geolocalizacion:\n",
    "      merge_geo = i(rutas[j], rutaestados)\n",
    "   elif i == etl_categoryproduct:\n",
    "      merge1 = i(rutas[j])\n",
    "   else:\n",
    "      i(rutas[j])\n",
    "\n",
    "   j = j + 1\n",
    "   p = p + 1\n",
    "   \n",
    "print('El proceso de normalización ha finalizado exitosamente')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['category_product.csv',\n",
       " 'closed_deal.csv',\n",
       " 'customer.csv',\n",
       " 'data_order.csv',\n",
       " 'data_review.csv',\n",
       " 'geolocation.csv',\n",
       " 'item.csv',\n",
       " 'payment.csv',\n",
       " 'product.csv',\n",
       " 'seller.csv']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "carpeta_data_norm = r'C:\\Users\\leand\\Desktop\\Data Science (Henry)\\Proyectos\\Proyecto_grupal\\DS-Proyecto_Grupal_Olist\\data_normalizada'\n",
    "lista_archivos = []\n",
    "for file in os.listdir(carpeta_data_norm):\n",
    "    if file.endswith('.csv'):\n",
    "        lista_archivos.append(file)\n",
    "\n",
    "lista_archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_archivos[1] = 'geolocation.csv'\n",
    "lista_archivos[2] = 'customer.csv'\n",
    "lista_archivos[3] = 'seller.csv'\n",
    "lista_archivos[4] = 'data_order.csv'\n",
    "lista_archivos[5] = 'product.csv'\n",
    "lista_archivos[6] = 'item.csv'\n",
    "lista_archivos[7] = 'payment.csv'\n",
    "lista_archivos[8] = 'data_review.csv'\n",
    "lista_archivos[9] = 'closed_deal.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n",
      "Carga subida a la DB\n"
     ]
    }
   ],
   "source": [
    "for archivo in lista_archivos:\n",
    "    my_file = open(carpeta_data_norm + '\\\\' + archivo)\n",
    "\n",
    "    try:\n",
    "        conn_string = \"host=%s dbname=%s user=%s password=%s\" % ('localhost', 'db_olist', 'postgres', '2685')\n",
    "        conn = psycopg2.connect(conn_string)\n",
    "        cursor = conn.cursor()\n",
    "        SQL_STATEMENT = \"\"\"COPY {} FROM stdin DELIMITER ',' CSV header;\"\"\".format(archivo[:-4])\n",
    "        cursor.copy_expert(sql=SQL_STATEMENT, file= my_file)\n",
    "        print('Carga subida a la DB')\n",
    "        conn.commit()\n",
    "    except (Exception, psycopg2.Error) as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DELTA ORDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % ('localhost', 'db_olist', 'postgres', '2685')\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"CREATE TABLE table_aux (order_id varchar, customer_id varchar, order_status varchar, order_purchase_timestamp timestamp,\\\n",
    "    order_approved_at timestamp, order_delivered_carrier_date timestamp, order_delivered_customer_date timestamp,\\\n",
    "    order_estimated_delivery_date timestamp, PRIMARY KEY (order_id));\")\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACÁ HAY QUE CREAR UNA FUNCIÓN DELTA QUE SEA IGUAL A ORDER, PERO RECIBA COMO PARÁMETRO LA RUTA DEL DELTA Y LUEGO LO EXPORTE A UN CSV order_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    #TRAIGO EL ARCHIVO DELTA NORMALIZADO\n",
    "    my_file = open(r\"C:\\Users\\leand\\Desktop\\Data Science (Henry)\\Proyectos\\Proyecto_grupal\\DS-Proyecto_Grupal_Olist\\data_normalizada\\data_order.csv\")\n",
    "\n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % ('localhost', 'db_olist', 'postgres', '2685')\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    SQL_STATEMENT = \"COPY table_aux FROM stdin DELIMITER ',' CSV header;\"\n",
    "    cursor.copy_expert(sql=SQL_STATEMENT, file= my_file)\n",
    "    \n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # INSERTAMOS LOS CUSTOMER_ID QUE SEAN NUEVOS EN LA TABLA CUSTOMER PARA NO PERDER LA INTEGRIDAD\n",
    "    \n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % ('localhost', 'db_olist', 'postgres', '2685')\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "\n",
    "    cursor.execute(\"INSERT INTO customer (customer_id)\\\n",
    "    SELECT customer_id FROM (SELECT * FROM table_aux\\\n",
    "    WHERE table_aux.customer_id NOT IN (SELECT customer_id FROM customer)) as pepito;\")\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inserción o actualización en la tabla «data_order» viola la llave foránea «data_order_fk_customer»\n",
      "DETAIL:  La llave (customer_id)=(9ef432eb6251297304e76186b10a928d) no está presente en la tabla «customer».\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # INSERTAMOS LOS AUXILIARES EN ORDENES DONDE NO SE REPITA ORDER_ID\n",
    "    \n",
    "    conn_string = \"host=%s dbname=%s user=%s password=%s\" % ('localhost', 'db_olist', 'postgres', '2685')\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "\n",
    "    cursor.execute(\"INSERT INTO data_order (order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date)\\\n",
    "    SELECT order_id, customer_id, order_status, order_purchase_timestamp, order_approved_at, order_delivered_carrier_date, order_delivered_customer_date, order_estimated_delivery_date FROM (SELECT * FROM table_aux\\\n",
    "    WHERE table_aux.order_id NOT IN (SELECT order_id FROM data_order)) as pepito;\")\n",
    "\n",
    "\n",
    "    conn.commit()\n",
    "\n",
    "except (Exception, psycopg2.Error) as error:\n",
    "    print(error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('datascience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9b0261dadc93494a3555537365c322d83416c4a1ed03d5df7c77bc94b07686c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
